# Dynamic-LoRA-MoE-TinyLlama
A parameter-efficient Mixture-of-Experts (MoE) system built on TinyLlama-1.1B. Features dynamic semantic routing between Code and Poetry experts, optimized for training on consumer hardware (4GB VRAM).
